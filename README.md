A. **Emotion Recognition**: Utilize OpenCV and DeepFace to analyze the user's facial expressions to recognize their emotions. The system will adjust the tone and content of responses based on the results of the emotion analysis to suit different emotional states such as happiness, sadness, anger, etc.

B. **Speech-to-Text**: Use Azure's Speech-to-Text API to convert the user's responses into text.

C. **Response with GPT**: Based on the emotion recognition results and the speech-to-text content, use the GPT-4o model to generate appropriate responses. This step combines the user's emotional state and voice input to generate more personalized and context-appropriate responses.

D. **Video Generation with D-ID**: Transform the responses generated by GPT-4o into videos using D-ID to create video content featuring characters. This makes the responses not just text-based but also includes facial expressions, enhancing the richness and appeal of the interaction.

`pip install -r requirements.txt`<br>
`python main.py`

